{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.248:4040\n",
       "SparkContext available as 'sc' (version = 3.0.0, master = local[*], app id = local-1600685088472)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n"
     ]
    }
   ],
   "source": [
    "println(\"init\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.feature.{StringIndexer, VectorIndexer, IndexToString}\n",
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.classification.{DecisionTreeClassifier, LogisticRegression, NaiveBayes, RandomForestClassifier}\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.feature.{StringIndexer, VectorIndexer, IndexToString}\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.classification.{DecisionTreeClassifier, LogisticRegression, NaiveBayes, RandomForestClassifier}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iris: org.apache.spark.sql.DataFrame = [sepal_length: double, sepal_width: double ... 3 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val iris = spark.read.format(\"csv\")\n",
    "                  .option(\"sep\", \",\")\n",
    "                  .option(\"inferSchema\", \"true\")\n",
    "                  .option(\"header\", \"true\")\n",
    "                  .load(\"/home/leon/data/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Seq[org.apache.spark.sql.types.DataType] = List(DoubleType, DoubleType, DoubleType, DoubleType, StringType)\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.schema.map(_.dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_1295dc8148b5, handleInvalid=error, numInputCols=4\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler()\n",
    "        .setInputCols(Array(\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"))\n",
    "        .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: Array[String] = Array(sepal_length, sepal_width, petal_length, petal_width, species)\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformed_data: org.apache.spark.sql.DataFrame = [species: string, features: vector]\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val transformed_data = assembler.transform(iris).drop(\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelIndexer: org.apache.spark.ml.feature.StringIndexerModel = StringIndexerModel: uid=strIdx_58fd09a27971, handleInvalid=error\n",
       "featureIndexer: org.apache.spark.ml.feature.VectorIndexerModel = VectorIndexerModel: uid=vecIdx_eb15290b58cc, numFeatures=4, handleInvalid=error\n",
       "labelConverter: org.apache.spark.ml.feature.IndexToString = idxToStr_ad74cfcdf12f\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_9c1ab826392c\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labelIndexer = new StringIndexer()\n",
    "        .setInputCol(\"species\")\n",
    "        .setOutputCol(\"indexedSpecies\")\n",
    "        .fit(transformed_data)\n",
    "        \n",
    "val featureIndexer = new VectorIndexer()\n",
    "        .setInputCol(\"features\")\n",
    "        .setOutputCol(\"indexedFeatures\")\n",
    "        .setMaxCategories(4)\n",
    "        .fit(transformed_data)\n",
    "        \n",
    "// Convert indexed labels back to original labels.\n",
    "val labelConverter = new IndexToString()\n",
    "        .setInputCol(\"prediction\")\n",
    "        .setOutputCol(\"predictedLabel\")\n",
    "        .setLabels(labelIndexer.labels)\n",
    "        \n",
    "// Declaring ML Pipeline \n",
    "val pipeline = new Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_2760f71f8189\n",
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_805275f027f0\n",
       "nb: org.apache.spark.ml.classification.NaiveBayes = nb_9977c8cae93b\n",
       "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_4db4134ce205\n",
       "res21: pipeline.type = pipeline_9c1ab826392c\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// Train a DecisionTree\n",
    "val dt: DecisionTreeClassifier = new DecisionTreeClassifier()\n",
    "        .setLabelCol(\"indexedSpecies\")\n",
    "        .setFeaturesCol(\"indexedFeatures\")\n",
    "pipeline.setStages(Array(labelIndexer, featureIndexer, dt, labelConverter))\n",
    "\n",
    "\n",
    "// Train the Logistic Regression.\n",
    "val lr: LogisticRegression = new LogisticRegression()\n",
    "        .setLabelCol(\"indexedSpecies\")\n",
    "        .setMaxIter(10)\n",
    "        .setRegParam(0.3)\n",
    "        .setElasticNetParam(0.8)\n",
    "        .setFamily(\"multinomial\")\n",
    "\n",
    "pipeline.setStages(Array(labelIndexer, featureIndexer, lr, labelConverter))\n",
    "\n",
    "\n",
    "// Train the Naive Bayes\n",
    "val nb: NaiveBayes = new NaiveBayes().setLabelCol(\"indexedSpecies\")\n",
    "pipeline.setStages(Array(labelIndexer, featureIndexer, nb, labelConverter))\n",
    "\n",
    "\n",
    "val rf: RandomForestClassifier = new RandomForestClassifier()\n",
    "        .setLabelCol(\"indexedSpecies\")\n",
    "        .setFeaturesCol(\"indexedFeatures\")\n",
    "        .setNumTrees(10)\n",
    "\n",
    "pipeline.setStages(Array(labelIndexer, featureIndexer, rf, labelConverter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.PipelineModel = pipeline_9c1ab826392c\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = pipeline.fit(transformed_data)\n",
    "\n",
    "// Saving Model to HDFS\n",
    "model.save(\"/home/leon/projects/epsilon-ensemble/models/iris_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
